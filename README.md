# ğŸ“‘ PDF-Inquire: Professional RAG System

**PDF-Inquire** ist ein modulares RAG-System (Retrieval-Augmented Generation), das prÃ¤zise Antworten auf Basis deiner lokalen Dokumente liefert. Durch die Kombination von **lokalen Embeddings** und **Cloud-basierten LLMs** bietet es die perfekte Balance zwischen Datenschutz, Geschwindigkeit und Kosteneffizienz.

----

## ğŸ’¡ Was ist RAG?

Standard-LLMs neigen zu "Halluzinationen", wenn sie Ã¼ber spezifische oder private Daten abgefragt werden. RAG lÃ¶st dieses Problem, indem es das Modell in einen **digitalen Bibliothekar** verwandelt:



* **Ingestion:** PDFs werden in semantische Fragmente (Chunks) zerlegt.
* **Retrieval:** Das System findet in Millisekunden die relevantesten Stellen fÃ¼r deine Frage.
* **Augmentation:** Das LLM erhÃ¤lt die Frage zusammen mit dem exakten Kontext.
* **Generation:** Die Antwort basiert faktentreu auf den bereitgestellten Daten.

---

## ğŸ— Architektur

Das System trennt strikt zwischen Datenvorbereitung und Abfrage-Logik:

### 1. Indexierungs-Pipeline (Offline)
* **Extraktion:** `PyMuPDF` extrahiert Text und Metadaten (Seitenzahlen, Dateinamen).
* **Chunking:** `RecursiveCharacterTextSplitter` nutzt ein Fenster von 500 Token mit 10% Overlap.
* **Embedding:** Lokale AusfÃ¼hrung via `sentence-transformers/all-MiniLM-L6-v2`.
* **Storage:** `ChromaDB` (persistentes SQLite-Backend).

### 2. Query-Pipeline (Online)
* **Semantic Search:** Vektorbasiert Suche nach den Top-k Ãœbereinstimmungen.
* **Prompt Engineering:** Spezialisierte System-Prompts erzwingen die Nutzung des Kontexts.
* **Response Generation:** `GPT-3.5-Turbo` (oder neuer) liefert die finale Antwort inklusive Quellenangaben.

---

## ğŸš€ Key Features

| Feature | Beschreibung |
| :--- | :--- |
| **Zero-Cost Embeddings** | Lokale HuggingFace-Modelle sparen API-Kosten und erhÃ¶hen den Datenschutz. |
| **Hybrid-Metadata** | Jede Antwort nennt Seite & Dateiname zur Verifizierung. |
| **Smart-Chunking** | Verhindert Informationsverlust durch intelligenten Text-Overlap. |
| **Persistence** | Einmal indexierte Dokumente bleiben dauerhaft gespeichert. |
| **Token Tracking** | Transparente Ãœbersicht der OpenAI-Kosten pro Session. |

---

## ğŸ›  Installation & Setup

### Voraussetzungen
* Python 3.10 oder hÃ¶her
* OpenAI API-Key

### Schritt-fÃ¼r-Schritt

1.  **Repository klonen:**
    ```bash
    git clone [https://github.com/dein-username/rag-system.git](https://github.com/dein-username/rag-system.git)
    cd rag-system
    ```

2.  **Virtuelle Umgebung einrichten:**
    ```bash
    python -m venv .venv
    # Windows
    .venv\Scripts\activate
    # macOS/Linux
    source .venv/bin/activate
    ```

3.  **AbhÃ¤ngigkeiten installieren:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Umgebungsvariablen konfigurieren:**
    Erstelle eine `.env` Datei im Hauptverzeichnis:
    ```bash
    OPENAI_API_KEY=dein_key_hier
    DB_PATH=./chroma_db
    DOCS_PATH=./data
    ```

---

## ğŸ“‚ Projektstruktur

```text
rag-system/
â”œâ”€â”€ data/               # Deine PDFs
â”œâ”€â”€ chroma_db/          # Persistenter Vektorspeicher
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion.py    # PDF Processing & Embedding
â”‚   â”œâ”€â”€ retrieval.py    # Suche & RAG Logik
â”‚   â””â”€â”€ app.py          # CLI oder UI Interface
â”œâ”€â”€ .env                # API Keys (nicht einchecken!)
â”œâ”€â”€ .gitignore          # SchlieÃŸt venv, .env und DB aus
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
