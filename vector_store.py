"""
vector_store.py â€“ Embedding-Berechnung und ChromaDB-Vektordatenbank.

WIE FUNKTIONIEREN VEKTOREN IN RAG?
Jeder Text-Chunk wird in einen hochdimensionalen Zahlenvektor umgewandelt
(z.B. 384 Dimensionen mit MiniLM). Ã„hnliche Texte haben Ã¤hnliche Vektoren.
Beim Suchen wird die Frage ebenfalls in einen Vektor umgewandelt und die
nÃ¤chsten Nachbarn im Vektorraum gefunden â€“ das ist semantische Suche.

Beispiel: "Auto" und "Fahrzeug" sind nahe beieinander im Vektorraum,
auch wenn keine WÃ¶rter Ã¼bereinstimmen (wie bei klassischer Volltextsuche).
"""

import hashlib
import logging
from typing import List, Optional, Tuple

from langchain.schema import Document
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from tqdm import tqdm

import config

logger = logging.getLogger(__name__)


class VectorStore:
    """
    Verwaltet die Vektordatenbank: Embeddings erstellen, speichern, suchen.

    ChromaDB wurde gewÃ¤hlt weil:
    - Persistent: Datenbank bleibt nach Programmende erhalten (kein erneutes Embedding)
    - Einfach: LÃ¤uft lokal, keine externe Infrastruktur nÃ¶tig
    - LangChain-Integration: Native UnterstÃ¼tzung
    - Skalierbar bis zu Millionen von Vektoren fÃ¼r ein Portfolio-Projekt mehr als genug
    """

    def __init__(
        self,
        persist_dir: str = config.CHROMA_PERSIST_DIR,
        collection_name: str = config.COLLECTION_NAME,
        embedding_model: str = config.EMBEDDING_MODEL,
    ) -> None:
        """
        Initialisiert den VectorStore und lÃ¤dt das Embedding-Modell.

        Das Embedding-Modell wird beim ersten Aufruf von HuggingFace heruntergeladen
        (~90MB fÃ¼r MiniLM) und danach lokal gecacht.

        Args:
            persist_dir: Verzeichnis fÃ¼r persistente ChromaDB-Daten.
            collection_name: Name der Kollektion in ChromaDB.
            embedding_model: HuggingFace Model-ID fÃ¼r Embeddings.
        """
        self.persist_dir = persist_dir
        self.collection_name = collection_name

        logger.info(f"ğŸ”¢ Lade Embedding-Modell: {embedding_model}")
        logger.info("  (Beim ersten Start: Download ~90MB, dann gecacht)")

        # HuggingFaceEmbeddings lÃ¤dt das Modell lokal â€“ kein API-Call, keine Kosten.
        # model_kwargs={"device": "cpu"} erzwingt CPU-Nutzung fÃ¼r KompatibilitÃ¤t.
        # Wer eine GPU hat: "cuda" einsetzen fÃ¼r ~10x schnellere Embedding-Berechnung.
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={"device": "cpu"},
            # normalize_embeddings=True verbessert die Cosine-Similarity-Berechnung
            encode_kwargs={"normalize_embeddings": True},
        )

        # ChromaDB-Instanz initialisieren (oder bestehende laden)
        self.db: Optional[Chroma] = self._load_or_create_db()
        logger.info("  âœ… VectorStore bereit")

    def _load_or_create_db(self) -> Chroma:
        """
        LÃ¤dt eine bestehende ChromaDB oder erstellt eine neue.

        Returns:
            ChromaDB-Instanz.
        """
        return Chroma(
            collection_name=self.collection_name,
            embedding_function=self.embeddings,
            persist_directory=self.persist_dir,
        )

    def add_documents(
        self, documents: List[Document], batch_size: int = 32
    ) -> int:
        """
        FÃ¼gt Dokumente zur Vektordatenbank hinzu, Ã¼berspringt Duplikate.

        PERFORMANCE-TIPP: Batch-Verarbeitung ist entscheidend!
        Einzelne Embeddings berechnen wÃ¤re ~10-50x langsamer als Batches,
        da das Modell fÃ¼r jeden Batch nur einmal geladen werden muss.

        Duplikat-Erkennung via Content-Hash verhindert, dass dieselbe PDF
        mehrfach indiziert wird (z.B. wenn das Programm zweimal mit der
        gleichen Datei aufgerufen wird).

        Args:
            documents: Liste von LangChain-Documents.
            batch_size: Anzahl der Dokumente pro Embedding-Batch.
                       32 ist ein guter Kompromiss zwischen Geschwindigkeit
                       und Speicherverbrauch.

        Returns:
            Anzahl der neu hinzugefÃ¼gten Dokumente.
        """
        # Bestehende Document-IDs abrufen, um Duplikate zu verhindern
        existing_ids = self._get_existing_ids()
        logger.debug(f"Bestehende Dokumente in DB: {len(existing_ids)}")

        # Nur neue Dokumente hinzufÃ¼gen (Content-Hash als eindeutige ID)
        new_docs = []
        new_ids = []
        skipped = 0

        for doc in documents:
            doc_id = self._compute_doc_id(doc)
            if doc_id in existing_ids:
                skipped += 1
                continue
            new_docs.append(doc)
            new_ids.append(doc_id)

        if skipped > 0:
            logger.info(f"  â­ï¸  {skipped} bereits vorhandene Chunks Ã¼bersprungen")

        if not new_docs:
            logger.info("  â„¹ï¸  Keine neuen Dokumente zum Indexieren")
            return 0

        logger.info(f"  ğŸ”¢ Berechne Embeddings fÃ¼r {len(new_docs)} Chunks...")

        # Batch-Verarbeitung mit Fortschrittsbalken
        # tqdm zeigt einen visuellen Fortschrittsbalken in der Konsole
        added_count = 0
        for i in tqdm(
            range(0, len(new_docs), batch_size),
            desc="  Embeddings",
            unit="batch",
            ncols=60,
        ):
            batch_docs = new_docs[i : i + batch_size]
            batch_ids = new_ids[i : i + batch_size]

            self.db.add_documents(documents=batch_docs, ids=batch_ids)
            added_count += len(batch_docs)

        logger.info(f"  âœ… {added_count} neue Chunks zur Vektordatenbank hinzugefÃ¼gt")
        return added_count

    def similarity_search(
        self,
        query: str,
        k: int = config.TOP_K_RESULTS,
        min_similarity: float = config.MIN_SIMILARITY,
    ) -> List[Tuple[Document, float]]:
        """
        Sucht die k Ã¤hnlichsten Chunks zur Suchanfrage.

        Die Suche funktioniert so:
        1. Frage â†’ Embedding-Vektor (gleicher Raum wie Dokument-Vektoren)
        2. Cosine-Similarity zwischen Frage-Vektor und allen Dokument-Vektoren
        3. Top-K mit hÃ¶chster Ã„hnlichkeit zurÃ¼ckgeben

        Args:
            query: Suchanfrage des Nutzers.
            k: Anzahl der zurÃ¼ckzugebenden Chunks.
            min_similarity: Mindest-Ã„hnlichkeit (0-1). Chunks darunter werden verworfen.

        Returns:
            Liste von (Document, similarity_score) Tupeln, absteigend nach Score.

        Raises:
            RuntimeError: Wenn die Datenbank leer ist.
        """
        if self.is_empty():
            raise RuntimeError(
                "Die Vektordatenbank ist leer. "
                "Bitte zuerst ein PDF mit --pdf indexieren."
            )

        # similarity_search_with_relevance_scores gibt (Document, score) Paare zurÃ¼ck
        # Score 1.0 = perfekte Ãœbereinstimmung, 0.0 = keine Ã„hnlichkeit
        results = self.db.similarity_search_with_relevance_scores(query=query, k=k)

        # Ergebnisse unter dem Schwellwert herausfiltern (Rauschen reduzieren)
        filtered = [
            (doc, score) for doc, score in results if score >= min_similarity
        ]

        if not filtered:
            logger.warning(
                f"Keine Chunks mit Similarity >= {min_similarity} gefunden. "
                f"Versuche eine andere Frageformulierung oder reduziere MIN_SIMILARITY."
            )

        return filtered

    def is_empty(self) -> bool:
        """
        PrÃ¼ft, ob die Vektordatenbank Dokumente enthÃ¤lt.

        Returns:
            True wenn leer, False wenn Dokumente vorhanden.
        """
        try:
            count = self.db._collection.count()
            return count == 0
        except Exception:
            return True

    def get_document_count(self) -> int:
        """
        Gibt die Anzahl der gespeicherten Chunks zurÃ¼ck.

        Returns:
            Anzahl der Chunks in der Datenbank.
        """
        try:
            return self.db._collection.count()
        except Exception:
            return 0

    def _get_existing_ids(self) -> set:
        """
        LÃ¤dt alle vorhandenen Document-IDs aus ChromaDB.

        Returns:
            Set von vorhandenen IDs.
        """
        try:
            result = self.db._collection.get(include=[])
            return set(result.get("ids", []))
        except Exception:
            return set()

    def _compute_doc_id(self, doc: Document) -> str:
        """
        Berechnet eine eindeutige ID fÃ¼r ein Dokument basierend auf seinem Inhalt.

        Warum Content-Hashing statt zufÃ¤lliger IDs?
        Bei identischem Inhalt ergibt sich dieselbe ID â†’ automatische Deduplizierung
        ohne eine separate Tracking-Datenbank zu pflegen.

        Args:
            doc: LangChain-Document-Objekt.

        Returns:
            MD5-Hash des Inhalts + Quelldatei als eindeutige ID.
        """
        content = f"{doc.metadata.get('source', '')}__{doc.page_content}"
        return hashlib.md5(content.encode("utf-8")).hexdigest()
